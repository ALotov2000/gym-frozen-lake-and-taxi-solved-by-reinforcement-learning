{"cells":[{"cell_type":"markdown","metadata":{"id":"d0W-WuYsjWPP"},"source":["# Artificial Intelligence Course - Fall 1402\n","## Computer Assignment #2 - Reinforcement Learning"]},{"cell_type":"markdown","metadata":{"id":"ksnYjMyNPAcn"},"source":["# Table of Contents\n","\n","- [Part 1: Value Iteration & Policy Iteration Algorithms](#1)\n","    - [َQuestion 1:](#1-0)\n","    - [َQuestion 2:](#1-1)\n","    - [َQuestion 3:](#1-12)\n","    - [َQuestion 4:](#1-2)\n","    - [َQuestion 5:](#1-3)\n","        - [Value Iteration](#1-3-1)\n","        - [Policy Iteration](#1-3-2)\n","    - [َQuestion 6:](#1-4)\n","        - [Value Iteration](#1-4-1)\n","        - [Policy Iteration](#1-4-2)\n","- [Part 2: Q-Learning Algorithm](#2)\n","    - [َQuestion 8:](#2-1)\n","    - [َQuestion 9:](#2-2)\n","    - [َQuestion 10:](#2-3)"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"dpTWKluXMHP5"},"outputs":[],"source":["# import\n","import numpy as np\n","import gymnasium as gym\n","from time import sleep\n","from typing import *"]},{"cell_type":"markdown","metadata":{"id":"EifP8FUKLXE7"},"source":["<a name='1'></a>\n","## Part 1: Value Iteration & Policy Iteration Algorithms"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["def render(env: gym.Env):\n","    env.render()\n","    sleep(0.1)"]},{"cell_type":"markdown","metadata":{"id":"iVJmGmCUnIGR"},"source":["<a name='1-0'></a>\n","### Question 1:"]},{"cell_type":"markdown","metadata":{"id":"xRDhQMwwnK2s"},"source":[]},{"cell_type":"markdown","metadata":{"id":"MO24LtBGLXZ7"},"source":["<a name='1-1'></a>\n","### Question 2:"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"VKP8LjK5jGoW"},"outputs":[],"source":["class ValueIteration:\n","    def __init__(self: Self, env: gym.Env, discount_factor: float, theta:float=1e-8):\n","        self.env = env\n","        self.discount_factor = discount_factor\n","        self.theta = theta\n","        self.state_values = np.zeros(self.env.observation_space.n)\n","        self.q_values = np.zeros((self.env.observation_space.n, \n","                    self.env.action_space.n))\n","\n","    def value_estimation(self: Self):\n","        env, state_values, q_values, discount_factor, theta = \\\n","                self.env, \\\n","                self.state_values, \\\n","                self.q_values, \\\n","                self.discount_factor, \\\n","                self.theta\n","            \n","        delta = np.inf\n","\n","        while(delta > theta):\n","            delta = 0\n","\n","            for state in range(env.observation_space.n - 1):\n","                previous_state_value = state_values[state]\n","\n","                for action in range(env.action_space.n):\n","                    action_value = 0\n","                    for probability, next_state, reward, _ in env.unwrapped.P[state][action]:\n","                        action_value += probability * \\\n","                                (reward + \\\n","                                discount_factor * state_values[next_state])\n","                    q_values[state, action] = action_value\n","\n","                state_values[state] = np.max(q_values[state,:])\n","\n","                delta = np.max([delta, abs(previous_state_value - state_values[state])])\n","\n","            if (delta < theta):\n","                break\n","\n","    def take_action(self: Self, action: Any):\n","        next_state, reward, terminated, _, _ = self.env.step(action)\n","        return next_state, reward, terminated\n","\n","    def get_optimal_policy(self: Self, state: Any):\n","        return np.argmax(self.q_values[state,:])\n","\n","    def get_state_values(self: Self):\n","        return self.state_values\n","\n","    def get_q_values(self: Self):\n","        return self.q_values\n","\n","    def reset(self: Self, *args, **kwargs):\n","        initial_state, _ = self.env.reset(*args, **kwargs)\n","        return initial_state, False"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YZARu6LDSqj_","outputId":"2f22167b-c6ef-493d-bec3-264a93e5d8cb"},"outputs":[{"name":"stdout","output_type":"stream","text":["State Utilities:\n"," [[0.06428821 0.05807365 0.07231299 0.05356057]\n"," [0.08830336 0.         0.11127288 0.        ]\n"," [0.14298808 0.24613328 0.29877497 0.        ]\n"," [0.         0.37905097 0.63860174 0.        ]]\n","Optimal Policy:\n"," [['L' 'T' 'L' 'T']\n"," ['L' 'L' 'L' 'L']\n"," ['T' 'B' 'L' 'L']\n"," ['L' 'R' 'B' 'L']]\n"]},{"name":"stdout","output_type":"stream","text":["Total Reward = 0.05233476330273609\n"]}],"source":["discount_factor = 0.9\n","env = gym.make('FrozenLake-v1', \n","               desc=None, \n","               map_name=\"4x4\", \n","               is_slippery=True,\n","               render_mode=\"human\")\n","value_iteration = ValueIteration(env, discount_factor, 0.001)\n","value_iteration.value_estimation()\n","\n","# print state_values and optimal_policy\n","action_char = ['L', 'B', 'R', 'T']\n","state_values = value_iteration.get_state_values().reshape(4, 4)\n","optimal_policy = np.array([action_char[value_iteration.get_optimal_policy(state)] for state in range(16)]).reshape(4, 4)\n","print(\"State Utilities:\\n\", state_values)\n","print(\"Optimal Policy:\\n\", optimal_policy)\n","\n","state, terminated = value_iteration.reset(seed=573)\n","total_reward = 0\n","move_index = 0\n","try:\n","    while not terminated:\n","        render(env)\n","        action = value_iteration.get_optimal_policy(state)\n","        state, reward, terminated = value_iteration.take_action(action)\n","        total_reward = total_reward + (discount_factor ** move_index) * reward\n","        move_index += 1\n","    render(env)\n","finally:\n","    print(f\"Total Reward = {total_reward}\")\n","    env.close()"]},{"cell_type":"markdown","metadata":{"id":"frjc5mR4ncm1"},"source":["<a name='1-12'></a>\n","### Question 3:"]},{"cell_type":"markdown","metadata":{"id":"AMJJpf1tnddF"},"source":[]},{"cell_type":"markdown","metadata":{"id":"V4DcH5yJLXqH"},"source":["<a name='1-2'></a>\n","### Question 4:"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":14,"status":"ok","timestamp":1698433256083,"user":{"displayName":"Mohammad Saadati","userId":"00242434153251678664"},"user_tz":-210},"id":"XjSb1lX147hd"},"outputs":[],"source":["class PolicyIteration():\n","    def __init__(self, env, discount_factor, theta=1e-8):\n","        self.env = env\n","        self.discount_factor = discount_factor\n","        self.theta = theta\n","        self.reset()\n","        self.state_values = np.ones((self.env.observation_space.n)) / self.env.action_space.n\n","        self.q_values = np.ones((self.env.observation_space.n, self.env.action_space.n)) / self.env.action_space.n\n","        self.state_values[self.env.observation_space.n - 1] = 0\n","        self.q_values[self.env.observation_space.n - 1] = np.zeros((self.env.action_space.n))\n","        self.policy = np.random.randint(self.env.action_space.n, size=self.env.observation_space.n) # initial policy\n","        self.policy_stable = False\n","\n","    def policy_evaluation(self):\n","        self.delta = np.inf\n","\n","        while(self.delta >= self.theta):\n","\n","            self.delta = 0\n","\n","            for state in range(self.env.observation_space.n):\n","\n","                v = self.state_values[state]\n","\n","                new_state_value = 0\n","                for probability, next_state, reward, done in self.env.P[state][self.policy[state]]:\n","                    ### START CODE HERE ###\n","                    new_state_value += ...\n","                    ### END CODE HERE ###\n","                self.state_values[state] = new_state_value\n","\n","                self.delta = np.max([self.delta, abs(v - self.state_values[state])])\n","\n","    def policy_improvement(self):\n","        self.policy_stable = True\n","\n","        for state in range(self.env.observation_space.n):\n","            old_policy = self.policy[state]\n","\n","            for action in range(self.env.action_space.n):\n","\n","                action_value = 0\n","                for probability, next_state, reward, done in self.env.P[state][action]:\n","                    ### START CODE HERE ###\n","                    action_value += ...\n","                    ### END CODE HERE ###\n","                self.q_values[state, action] = action_value\n","\n","            self.policy[state] = np.argmax(self.q_values[state,:])\n","\n","            if old_policy != self.policy[state]:\n","                self.policy_stable = False\n","\n","    def policy_estimation(self):\n","        self.policy_stable = False\n","\n","        while not self.policy_stable:\n","            self.policy_evaluation()\n","            self.policy_improvement()\n","\n","    def take_action(self, action):\n","        next_state, reward, done, _ = self.env.step(action)\n","        return next_state, reward, done\n","\n","    def get_optimal_policy(self, state):\n","        return self.policy[state]\n","\n","    def get_state_values(self):\n","        return self.state_values\n","\n","    def get_q_values(self):\n","        return self.q_values\n","\n","    def reset(self):\n","        initial_state = self.env.reset()\n","        return initial_state"]},{"cell_type":"markdown","metadata":{"id":"u4G-kVjmLYj4"},"source":["<a name='1-3'></a>\n","### Question 5:"]},{"cell_type":"markdown","metadata":{"id":"PB651-ZY4vjE"},"source":["<a name='1-3-1'></a>\n","#### Value Iteration:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RGYLOfYAuKjY"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"ipZlzoXH40Mn"},"source":["<a name='1-3-2'></a>\n","#### Policy Iteration:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7Vxf_xKc44QT"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"A3fnAFqJLpVI"},"source":["<a name='1-4'></a>\n","### Question 6:"]},{"cell_type":"markdown","metadata":{"id":"7JuoMPH_PAcv"},"source":["<a name='1-4-1'></a>\n","#### Value Iteration:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vCBnAicAPAcv"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"C1BMk-BfPAcv"},"source":["<a name='1-4-2'></a>\n","#### Policy Iteration:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LnUZpvLP446n"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"hLtPLm-ELpG9"},"source":["<a name='2'></a>\n","## Part 2: Q-Learning Algorithm"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"3cW_rkeDMOE8"},"outputs":[],"source":["# hyperparameters\n","REPS = 20\n","EPISODES = 2000\n","EPSILON = 0.1\n","LEARNING_RATE = 0.1\n","DISCOUNT = 0.9\n","STUDENT_NUM = 123"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"L1c1w7tRMOR_"},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/lotov2000/projects/UT/artificial-intelligence/ca02/.venv/lib/python3.12/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.seed to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.seed` for environment variables or `env.get_wrapper_attr('seed')` that will search the reminding wrappers.\u001b[0m\n","  logger.warn(\n"]},{"ename":"AttributeError","evalue":"'TaxiEnv' object has no attribute 'seed'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[1;32m/home/lotov2000/projects/UT/artificial-intelligence/ca02/src/main.ipynb Cell 27\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/lotov2000/projects/UT/artificial-intelligence/ca02/src/main.ipynb#X40sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# environment\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/lotov2000/projects/UT/artificial-intelligence/ca02/src/main.ipynb#X40sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m env \u001b[39m=\u001b[39m gym\u001b[39m.\u001b[39mmake(\u001b[39m'\u001b[39m\u001b[39mTaxi-v3\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/lotov2000/projects/UT/artificial-intelligence/ca02/src/main.ipynb#X40sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m env\u001b[39m.\u001b[39;49mseed(seed \u001b[39m=\u001b[39m STUDENT_NUM)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/lotov2000/projects/UT/artificial-intelligence/ca02/src/main.ipynb#X40sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m Initial_State \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mreset()\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/lotov2000/projects/UT/artificial-intelligence/ca02/src/main.ipynb#X40sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m Initial_State\n","File \u001b[0;32m~/projects/UT/artificial-intelligence/ca02/.venv/lib/python3.12/site-packages/gymnasium/core.py:315\u001b[0m, in \u001b[0;36mWrapper.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    310\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39maccessing private attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m is prohibited\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    311\u001b[0m logger\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    312\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39menv.\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m to get variables from other wrappers is deprecated and will be removed in v1.0, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    313\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mto get this variable you can do `env.unwrapped.\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m` for environment variables or `env.get_wrapper_attr(\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m)` that will search the reminding wrappers.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    314\u001b[0m )\n\u001b[0;32m--> 315\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv, name)\n","File \u001b[0;32m~/projects/UT/artificial-intelligence/ca02/.venv/lib/python3.12/site-packages/gymnasium/core.py:315\u001b[0m, in \u001b[0;36mWrapper.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    310\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39maccessing private attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m is prohibited\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    311\u001b[0m logger\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    312\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39menv.\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m to get variables from other wrappers is deprecated and will be removed in v1.0, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    313\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mto get this variable you can do `env.unwrapped.\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m` for environment variables or `env.get_wrapper_attr(\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m)` that will search the reminding wrappers.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    314\u001b[0m )\n\u001b[0;32m--> 315\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv, name)\n","File \u001b[0;32m~/projects/UT/artificial-intelligence/ca02/.venv/lib/python3.12/site-packages/gymnasium/core.py:315\u001b[0m, in \u001b[0;36mWrapper.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    310\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39maccessing private attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m is prohibited\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    311\u001b[0m logger\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    312\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39menv.\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m to get variables from other wrappers is deprecated and will be removed in v1.0, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    313\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mto get this variable you can do `env.unwrapped.\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m` for environment variables or `env.get_wrapper_attr(\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m)` that will search the reminding wrappers.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    314\u001b[0m )\n\u001b[0;32m--> 315\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv, name)\n","\u001b[0;31mAttributeError\u001b[0m: 'TaxiEnv' object has no attribute 'seed'"]}],"source":["# environment\n","env = gym.make('Taxi-v3')\n","env.seed(seed = STUDENT_NUM)\n","Initial_State = env.reset()\n","Initial_State"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ou0fWiX_MsZb"},"outputs":[],"source":["taxi_row, taxi_col, pass_idx, dest_idx = env.decode(Initial_State)\n","taxi_row, taxi_col, pass_idx, dest_idx"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-8tJoWefMOdT"},"outputs":[],"source":["# get familiar with the environment\n","print(\"you can see the environment in each step by render command :\")\n","env.render()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mQCB-ZIfNwJM"},"outputs":[],"source":["# Total no. of states\n","env.observation_space.n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lmk_EYbKNwYT"},"outputs":[],"source":["# Total no. of actions\n","env.action_space.n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Et2PlHqMMOoM"},"outputs":[],"source":["# base code for Q-learning\n","\n","env = gym.make('Taxi-v3')\n","env.seed(seed = STUDENT_NUM)\n","\n","\n","for rep in range(REPS):\n","    agent = # Agent Object instance from Algorithm_name(e.g Q_learning_agent) class which has inherited from Agentbase.\n","    for episode in range(EPISODES):\n","        Initial_state = env.reset()\n","\n","        for ... :\n","\n","            bestAction = np.random.choice(ACTIONS)\n","\n","            next_state,rew,done,_ = environment.step(bestAction)\n","\n","            if done:\n","                break"]},{"cell_type":"markdown","metadata":{"id":"oZJAP4nMLpiZ"},"source":["<a name='2-1'></a>\n","### Question 8:"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":503,"status":"ok","timestamp":1698434549406,"user":{"displayName":"Mohammad Saadati","userId":"00242434153251678664"},"user_tz":-210},"id":"Ue4m5fg9450B"},"outputs":[],"source":["class QLearningAgent():\n","    def __init__(self, env, epsilon, learning_rate, discount_factor, seed):\n","      self.env = env\n","      self.epsilon = epsilon\n","      self.learning_rate = learning_rate\n","      self.olr = learning_rate\n","      self.discount_factor = discount_factor\n","      self.q_table = np.zeros((env.observation_space.n, env.action_space.n))\n","      self.seed = seed\n","\n","    def choose_action(self, state):\n","      ### START CODE HERE ###\n","      # With probability epsilon, choose a random action\n","\n","      # Otherwise, choose the action with the highest Q-value\n","\n","      ### END CODE HERE ###\n","      return action\n","\n","    def update_q_table(self, state, action, nextState, reward):\n","      ### START CODE HERE ###\n","      # Calculate the new Q-value using the Q-learning formula\n","      self.q_table[state][action] = ...\n","      ### END CODE HERE ###\n","\n","    def decay_epsilon(self, episode):\n","      ### START CODE HERE ###\n","      self.epsilon = ...\n","      ### END CODE HERE ###\n","\n","    def decrease_learning_rate(self, episode):\n","      ### START CODE HERE ###\n","      self.learning_rate = ...\n","      ### END CODE HERE ###\n","\n","    def take_action(self, action):\n","      next_state, reward, done, _ = self.env.step(action)\n","      return next_state, reward, done\n","\n","    def get_optimal_policy(self, state):\n","      return np.argmax(self.q_table[state])\n","\n","    def get_q_values(self):\n","      return self.q_table\n","\n","    def reset(self):\n","      # self.learning_rate = self.olr\n","      return self.env.reset(seed=self.seed)"]},{"cell_type":"markdown","metadata":{"id":"c5HFAMk-Lpvs"},"source":["<a name='2-2'></a>\n","### Question 9:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qgRhXXmwo6MT"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"BVcMKEGDQWdU"},"source":["<a name='2-3'></a>\n","### Question 10:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FlHPV0kqQWdU"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.0"}},"nbformat":4,"nbformat_minor":0}
