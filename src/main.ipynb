{"cells":[{"cell_type":"markdown","metadata":{"id":"d0W-WuYsjWPP"},"source":["# Artificial Intelligence Course - Fall 1402\n","## Computer Assignment #2 - Reinforcement Learning"]},{"cell_type":"markdown","metadata":{"id":"ksnYjMyNPAcn"},"source":["# Table of Contents\n","\n","- [Part 1: Value Iteration & Policy Iteration Algorithms](#1)\n","    - [َQuestion 1:](#1-0)\n","    - [َQuestion 2:](#1-1)\n","    - [َQuestion 3:](#1-12)\n","    - [َQuestion 4:](#1-2)\n","    - [َQuestion 5:](#1-3)\n","        - [Value Iteration](#1-3-1)\n","        - [Policy Iteration](#1-3-2)\n","    - [َQuestion 6:](#1-4)\n","        - [Value Iteration](#1-4-1)\n","        - [Policy Iteration](#1-4-2)\n","- [Part 2: Q-Learning Algorithm](#2)\n","    - [َQuestion 8:](#2-1)\n","    - [َQuestion 9:](#2-2)\n","    - [َQuestion 10:](#2-3)"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"dpTWKluXMHP5"},"outputs":[],"source":["# import\n","import numpy as np\n","import gymnasium as gym\n","from time import sleep"]},{"cell_type":"markdown","metadata":{"id":"EifP8FUKLXE7"},"source":["<a name='1'></a>\n","## Part 1: Value Iteration & Policy Iteration Algorithms"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"8LizJeOYRMEq"},"outputs":[],"source":["env = gym.make('FrozenLake-v1', desc=None, map_name=\"4x4\", is_slippery=False, render_mode=\"human\")"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YZARu6LDSqj_","outputId":"2f22167b-c6ef-493d-bec3-264a93e5d8cb"},"outputs":[{"name":"stdout","output_type":"stream","text":["you can see the environment in each step by render command :\n"]},{"ename":"error","evalue":"display Surface quit","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)","\u001b[1;32m/home/lotov2000/projects/UT/artificial-intelligence/ca02/src/main.ipynb Cell 6\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/lotov2000/projects/UT/artificial-intelligence/ca02/src/main.ipynb#W5sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# get familiar with the environment\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/lotov2000/projects/UT/artificial-intelligence/ca02/src/main.ipynb#W5sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39myou can see the environment in each step by render command :\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/lotov2000/projects/UT/artificial-intelligence/ca02/src/main.ipynb#W5sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m env\u001b[39m.\u001b[39;49mreset(seed\u001b[39m=\u001b[39;49m\u001b[39m573\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/lotov2000/projects/UT/artificial-intelligence/ca02/src/main.ipynb#W5sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrender\u001b[39m():\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/lotov2000/projects/UT/artificial-intelligence/ca02/src/main.ipynb#W5sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     env\u001b[39m.\u001b[39mrender()\n","File \u001b[0;32m~/projects/UT/artificial-intelligence/ca02/.venv/lib/python3.12/site-packages/gymnasium/wrappers/time_limit.py:75\u001b[0m, in \u001b[0;36mTimeLimit.reset\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Resets the environment with :param:`**kwargs` and sets the number of steps elapsed to zero.\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \n\u001b[1;32m     68\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[39m    The reset environment\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     74\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_elapsed_steps \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m---> 75\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mreset(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n","File \u001b[0;32m~/projects/UT/artificial-intelligence/ca02/.venv/lib/python3.12/site-packages/gymnasium/wrappers/order_enforcing.py:61\u001b[0m, in \u001b[0;36mOrderEnforcing.reset\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Resets the environment with `kwargs`.\"\"\"\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_has_reset \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m---> 61\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mreset(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n","File \u001b[0;32m~/projects/UT/artificial-intelligence/ca02/.venv/lib/python3.12/site-packages/gymnasium/wrappers/env_checker.py:59\u001b[0m, in \u001b[0;36mPassiveEnvChecker.reset\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[39mreturn\u001b[39;00m env_reset_passive_checker(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m     58\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 59\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mreset(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n","File \u001b[0;32m~/projects/UT/artificial-intelligence/ca02/.venv/lib/python3.12/site-packages/gymnasium/envs/toy_text/frozen_lake.py:322\u001b[0m, in \u001b[0;36mFrozenLakeEnv.reset\u001b[0;34m(self, seed, options)\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlastaction \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    321\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrender_mode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mhuman\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 322\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrender()\n\u001b[1;32m    323\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mint\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39ms), {\u001b[39m\"\u001b[39m\u001b[39mprob\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m1\u001b[39m}\n","File \u001b[0;32m~/projects/UT/artificial-intelligence/ca02/.venv/lib/python3.12/site-packages/gymnasium/envs/toy_text/frozen_lake.py:338\u001b[0m, in \u001b[0;36mFrozenLakeEnv.render\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    336\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_render_text()\n\u001b[1;32m    337\u001b[0m \u001b[39melse\u001b[39;00m:  \u001b[39m# self.render_mode in {\"human\", \"rgb_array\"}:\u001b[39;00m\n\u001b[0;32m--> 338\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_render_gui(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrender_mode)\n","File \u001b[0;32m~/projects/UT/artificial-intelligence/ca02/.venv/lib/python3.12/site-packages/gymnasium/envs/toy_text/frozen_lake.py:408\u001b[0m, in \u001b[0;36mFrozenLakeEnv._render_gui\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    405\u001b[0m pos \u001b[39m=\u001b[39m (x \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcell_size[\u001b[39m0\u001b[39m], y \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcell_size[\u001b[39m1\u001b[39m])\n\u001b[1;32m    406\u001b[0m rect \u001b[39m=\u001b[39m (\u001b[39m*\u001b[39mpos, \u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcell_size)\n\u001b[0;32m--> 408\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwindow_surface\u001b[39m.\u001b[39;49mblit(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mice_img, pos)\n\u001b[1;32m    409\u001b[0m \u001b[39mif\u001b[39;00m desc[y][x] \u001b[39m==\u001b[39m \u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mH\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    410\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwindow_surface\u001b[39m.\u001b[39mblit(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhole_img, pos)\n","\u001b[0;31merror\u001b[0m: display Surface quit"]}],"source":["# get familiar with the environment\n","print(\"you can see the environment in each step by render command :\")\n","print(env.)\n","env.reset(seed=573)\n","def render():\n","    env.render()\n","    sleep(1)\n","render()\n","action = env.action_space.sample()\n","observation, reward, terminated, truncated, info = env.step(action)\n","render()\n","env.close()"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"x5QTTUSrSM-L","outputId":"206e612c-52ca-49d4-91b8-31f9faf5f1bc"},"outputs":[{"data":{"text/plain":["16"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["# Total no. of states\n","env.observation_space.n"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EeaBqUeZSNNY","outputId":"46e8a041-a1e4-4e43-c22a-4e2c1a3a3ddc"},"outputs":[{"data":{"text/plain":["4"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["# Total no. of actions\n","env.action_space.n"]},{"cell_type":"markdown","metadata":{"id":"iVJmGmCUnIGR"},"source":["<a name='1-0'></a>\n","### Question 1:"]},{"cell_type":"markdown","metadata":{"id":"xRDhQMwwnK2s"},"source":[]},{"cell_type":"markdown","metadata":{"id":"MO24LtBGLXZ7"},"source":["<a name='1-1'></a>\n","### Question 2:"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"VKP8LjK5jGoW"},"outputs":[],"source":["class ValueIteration():\n","    def __init__(self, env, discount_factor, theta=1e-8):\n","        self.env = env\n","        self.discount_factor = discount_factor\n","        self.theta = theta\n","        self.reset()\n","        self.state_values = np.ones((self.env.observation_space.n)) / self.env.action_space.n\n","        self.q_values = np.ones((self.env.observation_space.n, self.env.action_space.n)) / self.env.action_space.n\n","        self.state_values[self.env.observation_space.n - 1] = 0\n","        self.q_values[self.env.observation_space.n - 1] = np.zeros((self.env.action_space.n))\n","\n","    def value_estimation(self):\n","        self.delta = np.inf\n","\n","        while(self.delta > self.theta):\n","            self.delta = 0\n","\n","            for state in range(self.env.observation_space.n):\n","\n","                v = self.state_values[state]\n","\n","                for action in range(self.env.action_space.n):\n","                    action_value = 0\n","                    for probability, next_state, reward, done in self.env.P[state][action]:\n","                        ### START CODE HERE ###\n","                         action_value += ...\n","                        ### END CODE HERE ###\n","                    self.q_values[state, action] = action_value\n","\n","                self.state_values[state] = np.max(self.q_values[state,:])\n","\n","                self.delta = np.max([self.delta, abs(v - self.state_values[state])])\n","\n","                if (self.delta < self.theta):\n","                    break\n","\n","    def take_action(self, action):\n","        next_state, reward, done, _ = self.env.step(action)\n","        return next_state, reward, done\n","\n","    def get_optimal_policy(self, state):\n","        return np.argmax(self.q_values[state,:])\n","\n","    def get_state_values(self):\n","        return self.state_values\n","\n","    def get_q_values(self):\n","        return self.q_values\n","\n","    def reset(self):\n","        initial_state = self.env.reset()\n","        return initial_state"]},{"cell_type":"markdown","metadata":{"id":"frjc5mR4ncm1"},"source":["<a name='1-12'></a>\n","### Question 3:"]},{"cell_type":"markdown","metadata":{"id":"AMJJpf1tnddF"},"source":[]},{"cell_type":"markdown","metadata":{"id":"V4DcH5yJLXqH"},"source":["<a name='1-2'></a>\n","### Question 4:"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":14,"status":"ok","timestamp":1698433256083,"user":{"displayName":"Mohammad Saadati","userId":"00242434153251678664"},"user_tz":-210},"id":"XjSb1lX147hd"},"outputs":[],"source":["class PolicyIteration():\n","    def __init__(self, env, discount_factor, theta=1e-8):\n","        self.env = env\n","        self.discount_factor = discount_factor\n","        self.theta = theta\n","        self.reset()\n","        self.state_values = np.ones((self.env.observation_space.n)) / self.env.action_space.n\n","        self.q_values = np.ones((self.env.observation_space.n, self.env.action_space.n)) / self.env.action_space.n\n","        self.state_values[self.env.observation_space.n - 1] = 0\n","        self.q_values[self.env.observation_space.n - 1] = np.zeros((self.env.action_space.n))\n","        self.policy = np.random.randint(self.env.action_space.n, size=self.env.observation_space.n) # initial policy\n","        self.policy_stable = False\n","\n","    def policy_evaluation(self):\n","        self.delta = np.inf\n","\n","        while(self.delta >= self.theta):\n","\n","            self.delta = 0\n","\n","            for state in range(self.env.observation_space.n):\n","\n","                v = self.state_values[state]\n","\n","                new_state_value = 0\n","                for probability, next_state, reward, done in self.env.P[state][self.policy[state]]:\n","                    ### START CODE HERE ###\n","                    new_state_value += ...\n","                    ### END CODE HERE ###\n","                self.state_values[state] = new_state_value\n","\n","                self.delta = np.max([self.delta, abs(v - self.state_values[state])])\n","\n","    def policy_improvement(self):\n","        self.policy_stable = True\n","\n","        for state in range(self.env.observation_space.n):\n","            old_policy = self.policy[state]\n","\n","            for action in range(self.env.action_space.n):\n","\n","                action_value = 0\n","                for probability, next_state, reward, done in self.env.P[state][action]:\n","                    ### START CODE HERE ###\n","                    action_value += ...\n","                    ### END CODE HERE ###\n","                self.q_values[state, action] = action_value\n","\n","            self.policy[state] = np.argmax(self.q_values[state,:])\n","\n","            if old_policy != self.policy[state]:\n","                self.policy_stable = False\n","\n","    def policy_estimation(self):\n","        self.policy_stable = False\n","\n","        while not self.policy_stable:\n","            self.policy_evaluation()\n","            self.policy_improvement()\n","\n","    def take_action(self, action):\n","        next_state, reward, done, _ = self.env.step(action)\n","        return next_state, reward, done\n","\n","    def get_optimal_policy(self, state):\n","        return self.policy[state]\n","\n","    def get_state_values(self):\n","        return self.state_values\n","\n","    def get_q_values(self):\n","        return self.q_values\n","\n","    def reset(self):\n","        initial_state = self.env.reset()\n","        return initial_state"]},{"cell_type":"markdown","metadata":{"id":"u4G-kVjmLYj4"},"source":["<a name='1-3'></a>\n","### Question 5:"]},{"cell_type":"markdown","metadata":{"id":"PB651-ZY4vjE"},"source":["<a name='1-3-1'></a>\n","#### Value Iteration:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RGYLOfYAuKjY"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"ipZlzoXH40Mn"},"source":["<a name='1-3-2'></a>\n","#### Policy Iteration:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7Vxf_xKc44QT"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"A3fnAFqJLpVI"},"source":["<a name='1-4'></a>\n","### Question 6:"]},{"cell_type":"markdown","metadata":{"id":"7JuoMPH_PAcv"},"source":["<a name='1-4-1'></a>\n","#### Value Iteration:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vCBnAicAPAcv"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"C1BMk-BfPAcv"},"source":["<a name='1-4-2'></a>\n","#### Policy Iteration:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LnUZpvLP446n"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"hLtPLm-ELpG9"},"source":["<a name='2'></a>\n","## Part 2: Q-Learning Algorithm"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"3cW_rkeDMOE8"},"outputs":[],"source":["# hyperparameters\n","REPS = 20\n","EPISODES = 2000\n","EPSILON = 0.1\n","LEARNING_RATE = 0.1\n","DISCOUNT = 0.9\n","STUDENT_NUM = 123"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"L1c1w7tRMOR_"},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/lotov2000/projects/UT/artificial-intelligence/ca02/.venv/lib/python3.12/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.seed to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.seed` for environment variables or `env.get_wrapper_attr('seed')` that will search the reminding wrappers.\u001b[0m\n","  logger.warn(\n"]},{"ename":"AttributeError","evalue":"'TaxiEnv' object has no attribute 'seed'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[1;32m/home/lotov2000/projects/UT/artificial-intelligence/ca02/src/main.ipynb Cell 29\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/lotov2000/projects/UT/artificial-intelligence/ca02/src/main.ipynb#X40sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# environment\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/lotov2000/projects/UT/artificial-intelligence/ca02/src/main.ipynb#X40sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m env \u001b[39m=\u001b[39m gym\u001b[39m.\u001b[39mmake(\u001b[39m'\u001b[39m\u001b[39mTaxi-v3\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/lotov2000/projects/UT/artificial-intelligence/ca02/src/main.ipynb#X40sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m env\u001b[39m.\u001b[39;49mseed(seed \u001b[39m=\u001b[39m STUDENT_NUM)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/lotov2000/projects/UT/artificial-intelligence/ca02/src/main.ipynb#X40sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m Initial_State \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mreset()\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/lotov2000/projects/UT/artificial-intelligence/ca02/src/main.ipynb#X40sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m Initial_State\n","File \u001b[0;32m~/projects/UT/artificial-intelligence/ca02/.venv/lib/python3.12/site-packages/gymnasium/core.py:315\u001b[0m, in \u001b[0;36mWrapper.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    310\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39maccessing private attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m is prohibited\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    311\u001b[0m logger\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    312\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39menv.\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m to get variables from other wrappers is deprecated and will be removed in v1.0, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    313\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mto get this variable you can do `env.unwrapped.\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m` for environment variables or `env.get_wrapper_attr(\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m)` that will search the reminding wrappers.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    314\u001b[0m )\n\u001b[0;32m--> 315\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv, name)\n","File \u001b[0;32m~/projects/UT/artificial-intelligence/ca02/.venv/lib/python3.12/site-packages/gymnasium/core.py:315\u001b[0m, in \u001b[0;36mWrapper.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    310\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39maccessing private attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m is prohibited\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    311\u001b[0m logger\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    312\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39menv.\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m to get variables from other wrappers is deprecated and will be removed in v1.0, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    313\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mto get this variable you can do `env.unwrapped.\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m` for environment variables or `env.get_wrapper_attr(\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m)` that will search the reminding wrappers.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    314\u001b[0m )\n\u001b[0;32m--> 315\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv, name)\n","File \u001b[0;32m~/projects/UT/artificial-intelligence/ca02/.venv/lib/python3.12/site-packages/gymnasium/core.py:315\u001b[0m, in \u001b[0;36mWrapper.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    310\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39maccessing private attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m is prohibited\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    311\u001b[0m logger\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    312\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39menv.\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m to get variables from other wrappers is deprecated and will be removed in v1.0, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    313\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mto get this variable you can do `env.unwrapped.\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m` for environment variables or `env.get_wrapper_attr(\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m)` that will search the reminding wrappers.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    314\u001b[0m )\n\u001b[0;32m--> 315\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv, name)\n","\u001b[0;31mAttributeError\u001b[0m: 'TaxiEnv' object has no attribute 'seed'"]}],"source":["# environment\n","env = gym.make('Taxi-v3')\n","env.seed(seed = STUDENT_NUM)\n","Initial_State = env.reset()\n","Initial_State"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ou0fWiX_MsZb"},"outputs":[],"source":["taxi_row, taxi_col, pass_idx, dest_idx = env.decode(Initial_State)\n","taxi_row, taxi_col, pass_idx, dest_idx"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-8tJoWefMOdT"},"outputs":[],"source":["# get familiar with the environment\n","print(\"you can see the environment in each step by render command :\")\n","env.render()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mQCB-ZIfNwJM"},"outputs":[],"source":["# Total no. of states\n","env.observation_space.n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lmk_EYbKNwYT"},"outputs":[],"source":["# Total no. of actions\n","env.action_space.n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Et2PlHqMMOoM"},"outputs":[],"source":["# base code for Q-learning\n","\n","env = gym.make('Taxi-v3')\n","env.seed(seed = STUDENT_NUM)\n","\n","\n","for rep in range(REPS):\n","    agent = # Agent Object instance from Algorithm_name(e.g Q_learning_agent) class which has inherited from Agentbase.\n","    for episode in range(EPISODES):\n","        Initial_state = env.reset()\n","\n","        for ... :\n","\n","            bestAction = np.random.choice(ACTIONS)\n","\n","            next_state,rew,done,_ = environment.step(bestAction)\n","\n","            if done:\n","                break"]},{"cell_type":"markdown","metadata":{"id":"oZJAP4nMLpiZ"},"source":["<a name='2-1'></a>\n","### Question 8:"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":503,"status":"ok","timestamp":1698434549406,"user":{"displayName":"Mohammad Saadati","userId":"00242434153251678664"},"user_tz":-210},"id":"Ue4m5fg9450B"},"outputs":[],"source":["class QLearningAgent():\n","    def __init__(self, env, epsilon, learning_rate, discount_factor, seed):\n","      self.env = env\n","      self.epsilon = epsilon\n","      self.learning_rate = learning_rate\n","      self.olr = learning_rate\n","      self.discount_factor = discount_factor\n","      self.q_table = np.zeros((env.observation_space.n, env.action_space.n))\n","      self.seed = seed\n","\n","    def choose_action(self, state):\n","      ### START CODE HERE ###\n","      # With probability epsilon, choose a random action\n","\n","      # Otherwise, choose the action with the highest Q-value\n","\n","      ### END CODE HERE ###\n","      return action\n","\n","    def update_q_table(self, state, action, nextState, reward):\n","      ### START CODE HERE ###\n","      # Calculate the new Q-value using the Q-learning formula\n","      self.q_table[state][action] = ...\n","      ### END CODE HERE ###\n","\n","    def decay_epsilon(self, episode):\n","      ### START CODE HERE ###\n","      self.epsilon = ...\n","      ### END CODE HERE ###\n","\n","    def decrease_learning_rate(self, episode):\n","      ### START CODE HERE ###\n","      self.learning_rate = ...\n","      ### END CODE HERE ###\n","\n","    def take_action(self, action):\n","      next_state, reward, done, _ = self.env.step(action)\n","      return next_state, reward, done\n","\n","    def get_optimal_policy(self, state):\n","      return np.argmax(self.q_table[state])\n","\n","    def get_q_values(self):\n","      return self.q_table\n","\n","    def reset(self):\n","      # self.learning_rate = self.olr\n","      return self.env.reset(seed=self.seed)"]},{"cell_type":"markdown","metadata":{"id":"c5HFAMk-Lpvs"},"source":["<a name='2-2'></a>\n","### Question 9:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qgRhXXmwo6MT"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"BVcMKEGDQWdU"},"source":["<a name='2-3'></a>\n","### Question 10:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FlHPV0kqQWdU"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.0"}},"nbformat":4,"nbformat_minor":0}
