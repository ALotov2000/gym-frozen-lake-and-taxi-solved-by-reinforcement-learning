{"cells":[{"cell_type":"markdown","metadata":{"id":"d0W-WuYsjWPP"},"source":["# Artificial Intelligence Course - Fall 1402\n","## Computer Assignment #2 - Reinforcement Learning"]},{"cell_type":"markdown","metadata":{"id":"ksnYjMyNPAcn"},"source":["# Table of Contents\n","\n","- [Part 1: Value Iteration & Policy Iteration Algorithms](#1)\n","    - [َQuestion 1:](#1-0)\n","    - [َQuestion 2:](#1-1)\n","    - [َQuestion 3:](#1-12)\n","    - [َQuestion 4:](#1-2)\n","    - [َQuestion 5:](#1-3)\n","        - [Value Iteration](#1-3-1)\n","        - [Policy Iteration](#1-3-2)\n","    - [َQuestion 6:](#1-4)\n","        - [Value Iteration](#1-4-1)\n","        - [Policy Iteration](#1-4-2)\n","- [Part 2: Q-Learning Algorithm](#2)\n","    - [َQuestion 8:](#2-1)\n","    - [َQuestion 9:](#2-2)\n","    - [َQuestion 10:](#2-3)"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"dpTWKluXMHP5"},"outputs":[],"source":["# import\n","import numpy as np\n","import gymnasium as gym\n","from time import sleep, time\n","from typing import *"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["def monitor_time(f: Callable):\n","    start = time()\n","    result = f()\n","    end = time()\n","    return result, end - start"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["frozen_lake_discount_factor = 0.9\n","frozen_lake_env = gym.make('FrozenLake-v1', \n","               desc=None, \n","               map_name=\"8x8\", \n","               is_slippery=True,\n","               render_mode=\"rgb_array\")"]},{"cell_type":"markdown","metadata":{"id":"EifP8FUKLXE7"},"source":["<a name='1'></a>\n","## Part 1: Value Iteration & Policy Iteration Algorithms"]},{"cell_type":"markdown","metadata":{"id":"iVJmGmCUnIGR"},"source":["<a name='1-0'></a>\n","### Question 1:"]},{"cell_type":"markdown","metadata":{"id":"xRDhQMwwnK2s"},"source":[]},{"cell_type":"markdown","metadata":{"id":"MO24LtBGLXZ7"},"source":["<a name='1-1'></a>\n","### Question 2:"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"VKP8LjK5jGoW"},"outputs":[],"source":["\n","class ValueIteration:\n","    def __init__(self: Self, env: gym.Env, discount_factor: float, theta:float=1e-8):\n","        self.env = env\n","        self.discount_factor = discount_factor\n","        self.theta = theta\n","        self.state_values = np.zeros(env.observation_space.n)\n","        self.q_values = np.zeros((env.observation_space.n, \n","                    env.action_space.n))\n","        \n","        self.is_value_estimated = False\n","\n","    def value_estimation(self: Self):\n","        if self.is_value_estimated:\n","            return\n","        env, state_values, q_values, discount_factor, theta = \\\n","                self.env, \\\n","                self.state_values, \\\n","                self.q_values, \\\n","                self.discount_factor, \\\n","                self.theta\n","            \n","        delta = np.inf\n","\n","        while(delta > theta):\n","            delta = 0\n","\n","            for state in range(env.observation_space.n):\n","                previous_state_value = state_values[state]\n","\n","                for action in range(env.action_space.n):\n","                    action_value = 0\n","                    for probability, next_state, reward, _ in env.unwrapped.P[state][action]:\n","                        action_value += probability * \\\n","                                (reward + \\\n","                                discount_factor * state_values[next_state])\n","                    q_values[state, action] = action_value\n","\n","                state_values[state] = np.max(q_values[state,:])\n","\n","                delta = np.max([delta, abs(previous_state_value - state_values[state])])\n","                \n","        self.is_value_estimated = True\n","\n","    def take_action(self: Self, action: Any):\n","        next_state, reward, terminated, _, _ = self.env.step(action)\n","        return next_state, reward, terminated\n","\n","    def get_optimal_policy(self: Self, state: Any):\n","        return np.argmax(self.q_values[state,:])\n","\n","    def get_state_values(self: Self):\n","        return self.state_values\n","\n","    def get_q_values(self: Self):\n","        return self.q_values\n","\n","    def reset(self: Self, *args, **kwargs):\n","        initial_state, _ = self.env.reset(*args, **kwargs)\n","        return initial_state, False\n","    \n","    def auto(self: Self, seed:int=573, frame_freeze:float=0.02):\n","        env = self.env\n","\n","        self.value_estimation()\n","        state, terminated = self.reset(seed=seed)\n","        try:\n","            while not terminated:\n","                env.render() ; sleep(frame_freeze)\n","                action = self.get_optimal_policy(state)\n","                state, _, terminated = self.take_action(action)\n","            env.render() ; sleep(frame_freeze)\n","        finally:\n","            env.close()\n","            \n","    def __repr__(self: Self):\n","        env = self.env\n","        nrow, ncol = env.unwrapped.nrow, env.unwrapped.ncol\n","        n = env.observation_space.n\n","        \n","        self.value_estimation()\n","        to_format = np.vectorize(lambda x: f\"{x:.2f}\")\n","        state_values = to_format(self.get_state_values()).reshape(nrow, ncol)\n","        optimal_policy = np.array([self.get_optimal_policy(state) for state in range(n)]).reshape(nrow, ncol)\n","        st = \"\\n\".join([\"-\" * 10,\n","                \"State Utilities:\",\n","                str(state_values),\n","                \"Optimal Policy:\",\n","                str(optimal_policy),\n","                \"-\" * 10])\n","        return st\n","        \n","    __str__ = __repr__"]},{"cell_type":"markdown","metadata":{"id":"frjc5mR4ncm1"},"source":["<a name='1-12'></a>\n","### Question 3:"]},{"cell_type":"markdown","metadata":{"id":"AMJJpf1tnddF"},"source":[]},{"cell_type":"markdown","metadata":{"id":"V4DcH5yJLXqH"},"source":["<a name='1-2'></a>\n","### Question 4:"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":14,"status":"ok","timestamp":1698433256083,"user":{"displayName":"Mohammad Saadati","userId":"00242434153251678664"},"user_tz":-210},"id":"XjSb1lX147hd"},"outputs":[],"source":["class ModifiedPolicyIteration():\n","    def __init__(self: Self, env: gym.Env, discount_factor:float, theta:float=1e-8):\n","        self.env = env\n","        self.discount_factor = discount_factor\n","        self.theta = theta\n","        self.state_values = np.zeros((env.observation_space.n))\n","        self.q_values = np.zeros((env.observation_space.n, env.action_space.n))\n","        self.policy = np.random.randint(env.action_space.n, size=env.observation_space.n) # initial policy\n","        self.policy_stable = False\n","\n","    def policy_evaluation(self):\n","        env, discount_factor, theta, state_values, policy = self.env, \\\n","                self.discount_factor, \\\n","                self.theta, \\\n","                self.state_values, \\\n","                self.policy\n","        delta = np.inf\n","\n","        while(delta > theta):\n","\n","            delta = 0\n","\n","            for state in range(env.observation_space.n):\n","\n","                previous_state_value = state_values[state]\n","\n","                new_state_value = 0\n","                for probability, next_state, reward, _ in env.unwrapped.P[state][policy[state]]:\n","                    new_state_value += probability * \\\n","                                (reward + \\\n","                                discount_factor * state_values[next_state])\n","                state_values[state] = new_state_value\n","\n","                delta = np.max([delta, abs(previous_state_value - new_state_value)])\n","\n","    def policy_improvement(self: Self):\n","        env, discount_factor, state_values, q_values, policy = self.env, \\\n","                self.discount_factor, \\\n","                self.state_values, \\\n","                self.q_values, \\\n","                self.policy\n","        \n","        is_policy_stable = True\n","        for state in range(env.observation_space.n):\n","            old_policy = policy[state]\n","\n","            for action in range(env.action_space.n):\n","\n","                action_value = 0\n","                for probability, next_state, reward, _ in env.unwrapped.P[state][action]:\n","                    action_value += probability * \\\n","                                (reward + \\\n","                                discount_factor * state_values[next_state])\n","                q_values[state, action] = action_value\n","\n","            policy[state] = np.argmax(q_values[state,:])\n","\n","            if old_policy != policy[state]:\n","                is_policy_stable = False\n","        \n","        self.policy_stable = is_policy_stable\n","\n","    def policy_estimation(self: Self):\n","        \n","        while not self.policy_stable:\n","            self.policy_evaluation()\n","            self.policy_improvement()\n","\n","    def take_action(self: Self, action: Any):\n","        next_state, reward, terminated, _, _ = self.env.step(action)\n","        return next_state, reward, terminated\n","\n","    def get_optimal_policy(self: Self, state):\n","        return self.policy[state]\n","\n","    def get_state_values(self: Self):\n","        return self.state_values\n","\n","    def get_q_values(self: Self):\n","        return self.q_values\n","\n","    def reset(self: Self, *args, **kwargs):\n","        initial_state, _ = self.env.reset(*args, **kwargs)\n","        return initial_state, False\n","    \n","    def auto(self: Self, seed:int=573, frame_freeze:float=0.02):\n","        env = self.env\n","\n","        self.policy_estimation()\n","        state, terminated = self.reset(seed=seed)\n","        try:\n","            while not terminated:\n","                env.render() ; sleep(frame_freeze)\n","                action = self.get_optimal_policy(state)\n","                state, _, terminated = self.take_action(action)\n","            env.render() ; sleep(frame_freeze)\n","        finally:\n","            env.close()\n","            \n","    def __repr__(self: Self):\n","        env, policy = self.env, self.policy\n","        nrow, ncol = env.unwrapped.nrow, env.unwrapped.ncol\n","        n = env.observation_space.n\n","        \n","        self.policy_estimation()\n","        to_format = np.vectorize(lambda x: f\"{x:.2f}\")\n","        state_values = to_format(self.get_state_values()).reshape(nrow, ncol)\n","        optimal_policy = np.array(policy).reshape(nrow, ncol)\n","        st = \"\\n\".join([\"-\" * 10,\n","                \"State Utilities:\",\n","                str(state_values),\n","                \"Optimal Policy:\",\n","                str(optimal_policy),\n","                \"-\" * 10])\n","        return st\n","        \n","    __str__ = __repr__"]},{"cell_type":"markdown","metadata":{"id":"u4G-kVjmLYj4"},"source":["<a name='1-3'></a>\n","### Question 5:"]},{"cell_type":"markdown","metadata":{"id":"PB651-ZY4vjE"},"source":["<a name='1-3-1'></a>\n","#### Value Iteration:"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"RGYLOfYAuKjY"},"outputs":[{"name":"stdout","output_type":"stream","text":["value estimation took 0.18031716346740723s to process.\n","----------\n","State Utilities:\n","[['0.01' '0.01' '0.01' '0.02' '0.03' '0.03' '0.04' '0.04']\n"," ['0.01' '0.01' '0.01' '0.02' '0.03' '0.04' '0.05' '0.06']\n"," ['0.01' '0.01' '0.01' '0.00' '0.03' '0.04' '0.07' '0.08']\n"," ['0.00' '0.00' '0.01' '0.01' '0.02' '0.00' '0.09' '0.13']\n"," ['0.00' '0.00' '0.00' '0.00' '0.03' '0.06' '0.11' '0.21']\n"," ['0.00' '0.00' '0.00' '0.01' '0.03' '0.06' '0.00' '0.36']\n"," ['0.00' '0.00' '0.00' '0.00' '0.00' '0.12' '0.00' '0.63']\n"," ['0.00' '0.00' '0.00' '0.00' '0.14' '0.32' '0.61' '0.00']]\n","Optimal Policy:\n","[[3 2 2 2 2 2 2 2]\n"," [3 3 3 3 2 2 2 1]\n"," [3 3 0 0 2 3 2 1]\n"," [3 3 3 1 0 0 2 1]\n"," [3 3 0 0 2 1 3 2]\n"," [0 0 0 1 3 0 0 2]\n"," [0 0 1 0 0 0 0 2]\n"," [0 1 0 0 1 1 1 0]]\n","----------\n"]}],"source":["value_iteration = ValueIteration(frozen_lake_env, frozen_lake_discount_factor)\n","_, value_estimation_time = monitor_time(lambda: value_iteration.value_estimation())\n","print(f\"value estimation took {value_estimation_time}s to process.\")\n","print(value_iteration)\n","value_iteration.auto()"]},{"cell_type":"markdown","metadata":{"id":"ipZlzoXH40Mn"},"source":["<a name='1-3-2'></a>\n","#### Policy Iteration:"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"7Vxf_xKc44QT"},"outputs":[{"name":"stdout","output_type":"stream","text":["policy estimation took 0.24733185768127441s to process.\n","----------\n","State Utilities:\n","[['0.01' '0.01' '0.01' '0.02' '0.03' '0.03' '0.04' '0.04']\n"," ['0.01' '0.01' '0.01' '0.02' '0.03' '0.04' '0.05' '0.06']\n"," ['0.01' '0.01' '0.01' '0.00' '0.03' '0.04' '0.07' '0.08']\n"," ['0.00' '0.00' '0.01' '0.01' '0.02' '0.00' '0.09' '0.13']\n"," ['0.00' '0.00' '0.00' '0.00' '0.03' '0.06' '0.11' '0.21']\n"," ['0.00' '0.00' '0.00' '0.01' '0.03' '0.06' '0.00' '0.36']\n"," ['0.00' '0.00' '0.00' '0.00' '0.00' '0.12' '0.00' '0.63']\n"," ['0.00' '0.00' '0.00' '0.00' '0.14' '0.32' '0.61' '0.00']]\n","Optimal Policy:\n","[[3 2 2 2 2 2 2 2]\n"," [3 3 3 3 2 2 2 1]\n"," [3 3 0 0 2 3 2 1]\n"," [3 3 3 1 0 0 2 1]\n"," [3 3 0 0 2 1 3 2]\n"," [0 0 0 1 3 0 0 2]\n"," [0 0 1 0 0 0 0 2]\n"," [0 1 0 0 1 1 1 0]]\n","----------\n"]}],"source":["policy_iteration = ModifiedPolicyIteration(frozen_lake_env, frozen_lake_discount_factor)\n","_, policy_estimation_time = monitor_time(lambda: policy_iteration.policy_estimation())\n","print(f\"policy estimation took {policy_estimation_time}s to process.\")\n","print(policy_iteration)\n","policy_iteration.auto()"]},{"cell_type":"markdown","metadata":{"id":"A3fnAFqJLpVI"},"source":["<a name='1-4'></a>\n","### Question 6:"]},{"cell_type":"markdown","metadata":{"id":"7JuoMPH_PAcv"},"source":["<a name='1-4-1'></a>\n","#### Value Iteration:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vCBnAicAPAcv"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"C1BMk-BfPAcv"},"source":["<a name='1-4-2'></a>\n","#### Policy Iteration:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LnUZpvLP446n"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"hLtPLm-ELpG9"},"source":["<a name='2'></a>\n","## Part 2: Q-Learning Algorithm"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"3cW_rkeDMOE8"},"outputs":[],"source":["# hyperparameters\n","REPS = 20\n","EPISODES = 2000\n","EPSILON = 0.1\n","LEARNING_RATE = 0.1\n","DISCOUNT = 0.9\n","STUDENT_NUM = 123"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"L1c1w7tRMOR_"},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/lotov2000/projects/UT/artificial-intelligence/ca02/.venv/lib/python3.12/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.seed to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.seed` for environment variables or `env.get_wrapper_attr('seed')` that will search the reminding wrappers.\u001b[0m\n","  logger.warn(\n"]},{"ename":"AttributeError","evalue":"'TaxiEnv' object has no attribute 'seed'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[1;32m/home/lotov2000/projects/UT/artificial-intelligence/ca02/src/main.ipynb Cell 27\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/lotov2000/projects/UT/artificial-intelligence/ca02/src/main.ipynb#X35sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# environment\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/lotov2000/projects/UT/artificial-intelligence/ca02/src/main.ipynb#X35sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m env \u001b[39m=\u001b[39m gym\u001b[39m.\u001b[39mmake(\u001b[39m'\u001b[39m\u001b[39mTaxi-v3\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/lotov2000/projects/UT/artificial-intelligence/ca02/src/main.ipynb#X35sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m env\u001b[39m.\u001b[39;49mseed(seed \u001b[39m=\u001b[39m STUDENT_NUM)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/lotov2000/projects/UT/artificial-intelligence/ca02/src/main.ipynb#X35sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m Initial_State \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mreset()\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/lotov2000/projects/UT/artificial-intelligence/ca02/src/main.ipynb#X35sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m Initial_State\n","File \u001b[0;32m~/projects/UT/artificial-intelligence/ca02/.venv/lib/python3.12/site-packages/gymnasium/core.py:315\u001b[0m, in \u001b[0;36mWrapper.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    310\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39maccessing private attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m is prohibited\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    311\u001b[0m logger\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    312\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39menv.\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m to get variables from other wrappers is deprecated and will be removed in v1.0, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    313\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mto get this variable you can do `env.unwrapped.\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m` for environment variables or `env.get_wrapper_attr(\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m)` that will search the reminding wrappers.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    314\u001b[0m )\n\u001b[0;32m--> 315\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv, name)\n","File \u001b[0;32m~/projects/UT/artificial-intelligence/ca02/.venv/lib/python3.12/site-packages/gymnasium/core.py:315\u001b[0m, in \u001b[0;36mWrapper.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    310\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39maccessing private attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m is prohibited\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    311\u001b[0m logger\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    312\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39menv.\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m to get variables from other wrappers is deprecated and will be removed in v1.0, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    313\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mto get this variable you can do `env.unwrapped.\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m` for environment variables or `env.get_wrapper_attr(\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m)` that will search the reminding wrappers.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    314\u001b[0m )\n\u001b[0;32m--> 315\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv, name)\n","File \u001b[0;32m~/projects/UT/artificial-intelligence/ca02/.venv/lib/python3.12/site-packages/gymnasium/core.py:315\u001b[0m, in \u001b[0;36mWrapper.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    310\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39maccessing private attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m is prohibited\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    311\u001b[0m logger\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    312\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39menv.\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m to get variables from other wrappers is deprecated and will be removed in v1.0, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    313\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mto get this variable you can do `env.unwrapped.\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m` for environment variables or `env.get_wrapper_attr(\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m)` that will search the reminding wrappers.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    314\u001b[0m )\n\u001b[0;32m--> 315\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv, name)\n","\u001b[0;31mAttributeError\u001b[0m: 'TaxiEnv' object has no attribute 'seed'"]}],"source":["# environment\n","env = gym.make('Taxi-v3')\n","env.seed(seed = STUDENT_NUM)\n","Initial_State = env.reset()\n","Initial_State"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ou0fWiX_MsZb"},"outputs":[],"source":["taxi_row, taxi_col, pass_idx, dest_idx = env.decode(Initial_State)\n","taxi_row, taxi_col, pass_idx, dest_idx"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-8tJoWefMOdT"},"outputs":[],"source":["# get familiar with the environment\n","print(\"you can see the environment in each step by render command :\")\n","env.render()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mQCB-ZIfNwJM"},"outputs":[],"source":["# Total no. of states\n","env.observation_space.n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lmk_EYbKNwYT"},"outputs":[],"source":["# Total no. of actions\n","env.action_space.n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Et2PlHqMMOoM"},"outputs":[],"source":["# base code for Q-learning\n","\n","env = gym.make('Taxi-v3')\n","env.seed(seed = STUDENT_NUM)\n","\n","\n","for rep in range(REPS):\n","    agent = # Agent Object instance from Algorithm_name(e.g Q_learning_agent) class which has inherited from Agentbase.\n","    for episode in range(EPISODES):\n","        Initial_state = env.reset()\n","\n","        for ... :\n","\n","            bestAction = np.random.choice(ACTIONS)\n","\n","            next_state,rew,done,_ = environment.step(bestAction)\n","\n","            if done:\n","                break"]},{"cell_type":"markdown","metadata":{"id":"oZJAP4nMLpiZ"},"source":["<a name='2-1'></a>\n","### Question 8:"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":503,"status":"ok","timestamp":1698434549406,"user":{"displayName":"Mohammad Saadati","userId":"00242434153251678664"},"user_tz":-210},"id":"Ue4m5fg9450B"},"outputs":[],"source":["class QLearningAgent():\n","    def __init__(self, env, epsilon, learning_rate, discount_factor, seed):\n","      self.env = env\n","      self.epsilon = epsilon\n","      self.learning_rate = learning_rate\n","      self.olr = learning_rate\n","      self.discount_factor = discount_factor\n","      self.q_table = np.zeros((env.observation_space.n, env.action_space.n))\n","      self.seed = seed\n","\n","    def choose_action(self, state):\n","      ### START CODE HERE ###\n","      # With probability epsilon, choose a random action\n","\n","      # Otherwise, choose the action with the highest Q-value\n","\n","      ### END CODE HERE ###\n","      return action\n","\n","    def update_q_table(self, state, action, nextState, reward):\n","      ### START CODE HERE ###\n","      # Calculate the new Q-value using the Q-learning formula\n","      self.q_table[state][action] = ...\n","      ### END CODE HERE ###\n","\n","    def decay_epsilon(self, episode):\n","      ### START CODE HERE ###\n","      self.epsilon = ...\n","      ### END CODE HERE ###\n","\n","    def decrease_learning_rate(self, episode):\n","      ### START CODE HERE ###\n","      self.learning_rate = ...\n","      ### END CODE HERE ###\n","\n","    def take_action(self, action):\n","      next_state, reward, done, _ = self.env.step(action)\n","      return next_state, reward, done\n","\n","    def get_optimal_policy(self, state):\n","      return np.argmax(self.q_table[state])\n","\n","    def get_q_values(self):\n","      return self.q_table\n","\n","    def reset(self):\n","      # self.learning_rate = self.olr\n","      return self.env.reset(seed=self.seed)"]},{"cell_type":"markdown","metadata":{"id":"c5HFAMk-Lpvs"},"source":["<a name='2-2'></a>\n","### Question 9:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qgRhXXmwo6MT"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"BVcMKEGDQWdU"},"source":["<a name='2-3'></a>\n","### Question 10:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FlHPV0kqQWdU"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.0"}},"nbformat":4,"nbformat_minor":0}
